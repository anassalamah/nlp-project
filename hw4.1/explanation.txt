# Author: Anas Salamah
# Date: Oct 3, 2014

The first things I did was to strip the text from anything that is not a word. and that improved my predictions to around .69
After that I tried to get POS information to my features, realizing that that will need history since it is considering previous words. 
So I still decided to use POS on words independently but my laptop couldnt handle the processing and took a long time. This is what I wanted to do in the features function:

def features(self, text):
        d = defaultdict(int)
        for ii in wTOKENIZER.tokenize(text):
            d[morphy_stem(ii)] += 1
            d[nltk.pos_tag([morphy_stem(ii)]) += 1
            
I believe that would have pumbed my score at least by .1

So as an alternative, I looked for words that end with "ed" and labled them "VBD" and words ending with "ly" and labled them with "ADV" and that took me to .698.
Finally I looked for words starting with a capital letter and capture them too(that gave me a score of 0.7). I know that these are rule based queries that are not always correct but in this case, all the text fields will be evaluated in the same fasion and thus get a consistant feature lookup.
